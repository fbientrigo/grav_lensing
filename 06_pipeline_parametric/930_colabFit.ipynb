{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Bg-iV4SHnx_"
      },
      "source": [
        "Esta versión contiene las bases para entrenar modelos sin utilizar la función .fit la cual presenta leak de memoria.\n",
        "\n",
        "Se basa en las funciones que ya se encuentran presentes en la libreria\n",
        "\n",
        "Este notebook corresponde al caso en donde no comparamos pixeles de imagenes mientra se entrena, si no que es un approach completamente parametrico.\n",
        "\n",
        "El modelo si bien es capaz de generar las imagenes si se agrega una función extra de post-procesamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "L7v1Zi__Hnkc"
      },
      "outputs": [],
      "source": [
        "# !rm -r grav_lensing\n",
        "# !pip uninstall grav_lens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0hWPSbwlIOQi",
        "outputId": "4ea9c0cd-188c-44fb-ddf6-2d6fb261beb0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'grav_lensing'...\n",
            "remote: Enumerating objects: 1019, done.\u001b[K\n",
            "remote: Counting objects: 100% (79/79), done.\u001b[K\n",
            "remote: Compressing objects: 100% (56/56), done.\u001b[K\n",
            "remote: Total 1019 (delta 29), reused 68 (delta 22), pack-reused 940 (from 1)\u001b[K\n",
            "Receiving objects: 100% (1019/1019), 486.04 MiB | 21.94 MiB/s, done.\n",
            "Resolving deltas: 100% (449/449), done.\n",
            "Updating files: 100% (255/255), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/fbientrigo/grav_lensing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQ3gZ3H5hFnI",
        "outputId": "a7c05722-4e7b-4fac-ecf1-9e688be6b778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.4/13.4 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for grav_lens (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install /content/grav_lensing/src/ -qq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edhNmlQahQXz",
        "outputId": "95f3e308-cee7-4913-8685-26729aa2b4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-10-01 11:40:19--  https://descargas.inf.santiago.usm.cl/train/2.tar.gz\n",
            "Resolving descargas.inf.santiago.usm.cl (descargas.inf.santiago.usm.cl)... 200.1.22.243\n",
            "Connecting to descargas.inf.santiago.usm.cl (descargas.inf.santiago.usm.cl)|200.1.22.243|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8243980907 (7.7G) [application/octet-stream]\n",
            "Saving to: ‘2.tar.gz’\n",
            "\n",
            "2.tar.gz            100%[===================>]   7.68G  2.03MB/s    in 60m 38s \n",
            "\n",
            "2024-10-01 12:40:59 (2.16 MB/s) - ‘2.tar.gz’ saved [8243980907/8243980907]\n",
            "\n",
            "--2024-10-01 12:40:59--  http://./\n",
            "Resolving . (.)... failed: No address associated with hostname.\n",
            "wget: unable to resolve host address ‘.’\n",
            "FINISHED --2024-10-01 12:40:59--\n",
            "Total wall clock time: 1h 0m 40s\n",
            "Downloaded: 1 files, 7.7G in 1h 0m 38s (2.16 MB/s)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.makedirs('/content/data', exist_ok=True)\n",
        "!cd data && wget https://descargas.inf.santiago.usm.cl/train/2.tar.gz ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iYtuZzAuh0nW"
      },
      "outputs": [],
      "source": [
        "!tar -xf /content/data/2.tar.gz -C /content/data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_H-sbtKmPAg"
      },
      "outputs": [],
      "source": [
        "!ls /content/data/2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install visualkeras -qq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dQ2EQzed2QVt",
        "outputId": "8ebbf676-422d-45ff-b53a-ab557c40171b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/993.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.5/993.7 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.7/993.7 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pYt9zfpFgOP-",
        "outputId": "2c125703-63c7-4973-ad12-4115becb229f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<grav_lens.models.load_models.CustomMinMaxScaler object at 0x7912cd33ff10>\n",
            "IncrementalPCA(batch_size=64, n_components=64)\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import grav_lens.preprocess as gp\n",
        "from grav_lens.utils.statistics import get_stats\n",
        "from grav_lens.preprocess.filtering import process_batch_filters\n",
        "\n",
        "from grav_lens.models.load_models import load_minmaxscaler, load_ipca_low\n",
        "# Cargar el MinMaxScaler\n",
        "minmaxscaler = load_minmaxscaler()\n",
        "print(minmaxscaler)\n",
        "# Cargar el modelo IPCA low\n",
        "ipca_low = load_ipca_low()\n",
        "print(ipca_low)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZZp6tjoF0v7",
        "outputId": "e4cac786-5552-494a-bb17-d2c801667cbb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using data folder: /content/data/2\n",
            "(64, 128, 128, 3)\n",
            "(64, 128, 128, 1)\n"
          ]
        }
      ],
      "source": [
        "from grav_lens import get_datasets\n",
        "max_files = -1\n",
        "batch_size = 64\n",
        "home_data = \"/content/data\"\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = get_datasets(\n",
        "    data_index=2,\n",
        "    max_files=max_files,\n",
        "    home=home_data,\n",
        "    batch_size=batch_size,\n",
        "    val_split=0.2,\n",
        "    test_split=0.1,\n",
        ")\n",
        "\n",
        "for X_batch, y_batch in train_dataset.take(1):\n",
        "    print(X_batch.shape)\n",
        "    print(y_batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rCS7M1AlggCJ"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duy0LMPcghl0"
      },
      "source": [
        "### Inicialización"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LvnJ_Zeu3fNz"
      },
      "outputs": [],
      "source": [
        "N_GAUSS = 40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0rHp7IjgjLD",
        "outputId": "1a56800f-6969-4c63-c3a7-1505bed43e8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 128, 128, 3)\n",
            "(64, 128, 128, 1)\n",
            "(64, 64)\n",
            "(64, 40, 5)\n"
          ]
        }
      ],
      "source": [
        "def preprocess_ybatch(y_batch, corte=2):\n",
        "    low_batch, high_batch = process_batch_filters(y_batch)\n",
        "\n",
        "    # se agregaron tecnicas de regularización\n",
        "    gaussians = gp.gmm_batch_vectors(high_batch, n_gaussians_positive=N_GAUSS, n_gaussians_negative=0,\n",
        "                                    threshold=corte, n_points=750, scale=0.75, density_threshold=0.01)\n",
        "\n",
        "    low_freq_stack = np.vstack([img.reshape(-1, 128*128) for img in low_batch]) #stack para el pca\n",
        "    principal_components = ipca_low.transform(low_freq_stack)\n",
        "    return [principal_components, gaussians]\n",
        "\n",
        "\n",
        "\n",
        "for X_batch, y_batch in train_dataset.take(1):\n",
        "    y_batch = minmaxscaler.transform(y_batch) # [0, 1] #escalar los datos\n",
        "\n",
        "    print(X_batch.shape)\n",
        "    print(y_batch.shape)\n",
        "\n",
        "    y_process = preprocess_ybatch(y_batch)\n",
        "    print(y_process[0].shape)\n",
        "    print(y_process[1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJllx9Y3gmpu"
      },
      "source": [
        "### Creacion del modelo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "\n",
        "# Bloque convolucional estándar con BatchNormalization y ReLU\n",
        "def conv_block(x, filters, kernel_size=(3, 3), padding='same', use_batchnorm=True):\n",
        "    x = layers.Conv2D(filters, kernel_size, padding=padding, kernel_initializer='he_normal')(x)\n",
        "    if use_batchnorm:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "    return x\n",
        "\n",
        "# Bloque residual\n",
        "def residual_block(x, filters, activation='relu', use_batchnorm=True):\n",
        "    shortcut = x  # Conexión residual\n",
        "\n",
        "    # Primer bloque convolucional\n",
        "    x = conv_block(x, filters, kernel_size=(3, 3), use_batchnorm=use_batchnorm)\n",
        "\n",
        "    # Segundo bloque convolucional\n",
        "    x = layers.Conv2D(filters, (3, 3), padding='same', kernel_initializer='he_normal')(x)\n",
        "    if use_batchnorm:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # Adaptar el número de canales del atajo (shortcut) si es necesario\n",
        "    if shortcut.shape[-1] != filters:\n",
        "        shortcut = layers.Conv2D(filters, (1, 1), padding='same', kernel_initializer='he_normal')(shortcut)\n",
        "\n",
        "    # Agregar la conexión residual y activar\n",
        "    x = layers.Add()([x, shortcut])\n",
        "    x = layers.Activation(activation)(x)\n",
        "    return x\n",
        "\n",
        "# Bloque convolucional denso\n",
        "def dense_block(x, units, dropout_rate=0.05, activation='relu',use_batchnorm=True):\n",
        "    x = layers.Dense(units, kernel_initializer='he_normal')(x)\n",
        "    if use_batchnorm:\n",
        "        x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(activation)(x)\n",
        "    x = layers.Dropout(dropout_rate)(x)\n",
        "    return x\n"
      ],
      "metadata": {
        "id": "VEXHuVvW2xZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d0qJPVWWgnvO"
      },
      "outputs": [],
      "source": [
        "# Función para construir el modelo CNN con bloques modulares\n",
        "def create_complex_cnn_model(input_shape=(128, 128, 3), n_gaussians=40, dropout_rate=0.05):\n",
        "    input_img = layers.Input(shape=input_shape)\n",
        "    trimmed_input = layers.Lambda(lambda x: x[..., :2])(input_img)  # Seleccionar los primeros dos canales\n",
        "\n",
        "    # Bloques convolucionales iniciales\n",
        "    x = conv_block(trimmed_input, 32)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "    x = conv_block(x, 64)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Bloque convolucional con conexiones residuales\n",
        "    x = residual_block(x, 64)\n",
        "    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "    # Rama de baja frecuencia (lowfreq) - predicción de PCA\n",
        "    low_freq = conv_block(x, 128)\n",
        "    low_freq = layers.MaxPooling2D(pool_size=(2, 2))(low_freq)\n",
        "    low_freq = layers.Flatten()(low_freq)\n",
        "    low_freq = dense_block(low_freq, 256, dropout_rate=dropout_rate)\n",
        "    low_freq = layers.Dense(64, name='lowfreq_output')(low_freq)\n",
        "    low_freq_output = layers.LeakyReLU(alpha=0.2)(low_freq)\n",
        "\n",
        "    # Rama de alta frecuencia (highfreq) - predicción de gaussianas\n",
        "    high_freq = residual_block(x, 64)\n",
        "    high_freq = layers.MaxPooling2D(pool_size=(2, 2))(high_freq)\n",
        "    high_freq = layers.Flatten()(high_freq)\n",
        "    high_freq = dense_block(high_freq, 256, dropout_rate=dropout_rate)\n",
        "\n",
        "    high_freq_output = layers.Dense(n_gaussians * 5, activation='linear', name='highfreq_output')(high_freq)\n",
        "    high_freq_output = layers.Reshape((n_gaussians, 5))(high_freq_output)\n",
        "\n",
        "    # Escalado de la salida de alta frecuencia\n",
        "    mean_x_y = layers.Activation('tanh')(high_freq_output[..., :2])\n",
        "    mean_x_y = layers.Lambda(lambda x: (128/2)*(x + 1) , name='scaled_mean_x_y')(mean_x_y)\n",
        "\n",
        "    std_x_y = layers.Activation('sigmoid')(high_freq_output[..., 2:4])\n",
        "    std_x_y = layers.Lambda(lambda x: x * 5 + 1e-4, name='scaled_std_x_y')(std_x_y)\n",
        "\n",
        "    weights = layers.Activation('relu')(high_freq_output[..., 4])\n",
        "    high_freq_output_scaled = layers.Concatenate(axis=-1)([mean_x_y, std_x_y, weights[..., tf.newaxis]])\n",
        "\n",
        "    # Modelo final con las dos ramas de salida\n",
        "    model = models.Model(inputs=input_img, outputs=[low_freq_output, high_freq_output_scaled])\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "BcPibj7Og0IM"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# callbacks\n",
        "def prepare_callbacks(callbacks, model):\n",
        "    if callbacks:\n",
        "        for callback in callbacks:\n",
        "            callback.set_model(model)\n",
        "            callback.on_train_begin()\n",
        "    return callbacks\n",
        "\n",
        "def run_callbacks_on_epoch_end(callbacks, epoch, logs):\n",
        "    if callbacks:\n",
        "        for callback in callbacks:\n",
        "            callback.on_epoch_end(epoch, logs=logs)\n",
        "\n",
        "def run_callbacks_on_train_end(callbacks):\n",
        "    if callbacks:\n",
        "        for callback in callbacks:\n",
        "            callback.on_train_end()\n",
        "\n",
        "\n",
        "\n",
        "# Función para inicializar el optimizador con un scheduler\n",
        "def get_optimizer_with_scheduler(optimizer_name='sgd', initial_lr=0.001, scheduler=None, momentum=0.0, nesterov=False):\n",
        "    \"\"\"\n",
        "    Inicializa el optimizador con un scheduler opcional.\n",
        "\n",
        "    Parámetros:\n",
        "        optimizer_name (str): Nombre del optimizador a utilizar ('adam' o 'sgd').\n",
        "        initial_lr (float): Learning rate inicial.\n",
        "        scheduler (tf.keras.optimizers.schedules, opcional): Scheduler para ajustar el learning rate durante el entrenamiento.\n",
        "        momentum (float, opcional): Momento para el optimizador SGD (por defecto 0.0).\n",
        "        nesterov (bool, opcional): Si usar Nesterov momentum para SGD (por defecto False).\n",
        "\n",
        "    Retorna:\n",
        "        tf.keras.optimizers.Optimizer: Optimizador inicializado.\n",
        "    \"\"\"\n",
        "    # Si se proporciona un scheduler, usarlo como learning rate\n",
        "    lr = scheduler if scheduler else initial_lr\n",
        "\n",
        "    if optimizer_name.lower() == 'adam':\n",
        "        return tf.keras.optimizers.Adam(learning_rate=lr)\n",
        "    elif optimizer_name.lower() == 'sgd':\n",
        "        return tf.keras.optimizers.SGD(learning_rate=lr, momentum=momentum, nesterov=nesterov)\n",
        "    else:\n",
        "        raise ValueError(\"Optimizer not recognized. Please use 'adam' or 'sgd'.\")\n",
        "# ------\n",
        "\n",
        "\n",
        "# ..... regularización\n",
        "import tensorflow as tf\n",
        "\n",
        "def std_regularization_loss(std_x, std_y, lambda_reg=0.5):\n",
        "    \"\"\"\n",
        "    Calcula una pérdida de regularización que incentiva la diversidad de las desviaciones estándar.\n",
        "\n",
        "    Parámetros:\n",
        "        std_x (tf.Tensor): Tensor de desviaciones estándar `x`.\n",
        "        std_y (tf.Tensor): Tensor de desviaciones estándar `y`.\n",
        "        lambda_reg (float): Factor de regularización que controla la magnitud de la penalización.\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Valor escalar de la pérdida de regularización.\n",
        "    \"\"\"\n",
        "    # Calcular la media y la varianza de std_x y std_y\n",
        "    mean_std_x = tf.reduce_mean(std_x)\n",
        "    mean_std_y = tf.reduce_mean(std_y)\n",
        "\n",
        "    var_std_x = tf.math.reduce_variance(std_x)\n",
        "    var_std_y = tf.math.reduce_variance(std_y)\n",
        "\n",
        "    # La pérdida de regularización busca aumentar la varianza y alejar los valores de la media\n",
        "    reg_loss_x = tf.math.maximum(0.0, 1.0 - var_std_x)  # Penaliza poca varianza\n",
        "    reg_loss_y = tf.math.maximum(0.0, 1.0 - var_std_y)\n",
        "\n",
        "    # Sumar las pérdidas de regularización para std_x y std_y\n",
        "    reg_loss = lambda_reg * (reg_loss_x + reg_loss_y)\n",
        "\n",
        "    return reg_loss\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "def mean_regularization_loss(mean_x, mean_y, alpha_reg=0.5, beta_reg=0.01, target_mean=64.0):\n",
        "    \"\"\"\n",
        "    Calcula una pérdida de regularización que incentiva la diversidad de las medias.\n",
        "\n",
        "    Parámetros:\n",
        "        mean_x (tf.Tensor): Tensor de medias `x`.\n",
        "        mean_y (tf.Tensor): Tensor de medias `y`.\n",
        "        alpha_reg (float): Factor de regularización que controla la penalización de la dispersión.\n",
        "        beta_reg (float): Factor de regularización que controla la penalización respecto al objetivo `target_mean`.\n",
        "        target_mean (float): Valor objetivo para las medias (por defecto 64.0, la mitad de 128).\n",
        "\n",
        "    Returns:\n",
        "        tf.Tensor: Valor escalar de la pérdida de regularización.\n",
        "    \"\"\"\n",
        "\n",
        "    # Calcular la media de las medias\n",
        "    mean_mean_x = tf.reduce_mean(mean_x)\n",
        "    mean_mean_y = tf.reduce_mean(mean_y)\n",
        "\n",
        "    # Penalizar la cercanía de las medias al objetivo `target_mean`\n",
        "    mean_deviation_x = -tf.abs(mean_mean_x - target_mean)\n",
        "    mean_deviation_y = -tf.abs(mean_mean_y - target_mean)\n",
        "\n",
        "    # Penalizar la falta de varianza\n",
        "    var_m_x = tf.math.reduce_variance(mean_x)\n",
        "    var_m_y = tf.math.reduce_variance(mean_y)\n",
        "    reg_loss_var_x = tf.math.maximum(0.0, 1.0 - var_m_x)  # Penaliza poca varianza\n",
        "    reg_loss_var_y = tf.math.maximum(0.0, 1.0 - var_m_y)\n",
        "\n",
        "    # Sumar las pérdidas de regularización para incentivar diversidad y alejarse del target_mean\n",
        "    reg_loss = alpha_reg * (reg_loss_var_x + reg_loss_var_y) - beta_reg * (mean_deviation_x + mean_deviation_y)\n",
        "\n",
        "    return reg_loss\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#------\n",
        "\n",
        "# Definir un scheduler personalizado (opcional)\n",
        "def cosine_scheduler(epoch, lr):\n",
        "    max_epochs = 30\n",
        "    return lr * (tf.math.cos(epoch * np.pi / max_epochs) + 1) / 2\n",
        "\n",
        "# Función para calcular la pérdida y las métricas (solo MSE en este caso)\n",
        "def calculate_losses(y_true, y_pred, mse_fn):\n",
        "    mse_loss = mse_fn(y_true, y_pred)\n",
        "    return mse_loss\n",
        "\n",
        "# Función para ejecutar el entrenamiento de una época\n",
        "def train_epoch(model, train_dataset, optimizer, mse_fn, print_steps,\n",
        "                mean_regularization={'alpha_reg': 5, 'beta_reg': 0.5, 'target_mean': 64.0},\n",
        "                std_regularization={'lambda_reg': 5}):\n",
        "\n",
        "    train_mse_metric = tf.keras.metrics.Mean(name='train_mse')\n",
        "    lr_metric = tf.keras.metrics.Mean(name='learning_rate')\n",
        "\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        # Preprocesar y_batch\n",
        "        principal_components, gaussians = preprocess_ybatch(y_batch_train)\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            # El modelo debe devolver las dos ramas: lowfreq y highfreq\n",
        "            lowfreq_pred, highfreq_pred = model(x_batch_train, training=True)\n",
        "\n",
        "            # Calcular pérdidas separadas para ambas ramas\n",
        "            mse_loss_low = mse_fn(principal_components, lowfreq_pred)\n",
        "            mse_loss_high = mse_fn(gaussians, highfreq_pred)\n",
        "\n",
        "            # Extraer los valores de mean_x y mean_y\n",
        "            mean_x_pred = highfreq_pred[..., 0]\n",
        "            mean_y_pred = highfreq_pred[..., 1]\n",
        "\n",
        "            # Extraer los valores de std_x y std_y\n",
        "            std_x_pred = highfreq_pred[..., 2]\n",
        "            std_y_pred = highfreq_pred[..., 3]\n",
        "\n",
        "            # Calcular la pérdida de regularización para las desviaciones estándar\n",
        "            # Utiliza el diccionario de std_regularization\n",
        "            reg_loss_std = std_regularization_loss(std_x_pred, std_y_pred,\n",
        "                                                   lambda_reg=std_regularization.get('lambda_reg', 0.5))\n",
        "\n",
        "            # Calcular la pérdida de regularización para las medias\n",
        "            # Utiliza el diccionario de mean_regularization\n",
        "            reg_loss_mean = mean_regularization_loss(\n",
        "                mean_x_pred, mean_y_pred,\n",
        "                alpha_reg=mean_regularization.get('alpha_reg', 0.5),\n",
        "                beta_reg=mean_regularization.get('beta_reg', 0.01),\n",
        "                target_mean=mean_regularization.get('target_mean', 64.0))\n",
        "\n",
        "            # Sumar las pérdidas de regularización\n",
        "            reg_loss = reg_loss_std + reg_loss_mean\n",
        "\n",
        "            # Sumar las pérdidas\n",
        "            total_loss = mse_loss_low + mse_loss_high + reg_loss\n",
        "\n",
        "        # Aplicar gradientes\n",
        "        grads = tape.gradient(total_loss, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Actualizar la métrica MSE\n",
        "        train_mse_metric(total_loss)\n",
        "\n",
        "        # Capturar el learning rate actual y actualizar la métrica\n",
        "        lr_metric(optimizer.learning_rate.numpy() if isinstance(optimizer.learning_rate, tf.Tensor) else optimizer.learning_rate)\n",
        "\n",
        "        # Mostrar progreso\n",
        "        if step % print_steps == 0:\n",
        "            print(f'Step {step}: Total MSE = {train_mse_metric.result().numpy():.3f}, \\tLowFreq MSE = {mse_loss_low.numpy():.3f}, \\tHighFreq MSE = {mse_loss_high.numpy():.3f}, \\tReg Loss = {reg_loss.numpy():.3f}, \\tLR = {lr_metric.result().numpy():.3e}')\n",
        "\n",
        "    return train_mse_metric.result().numpy(), lr_metric.result().numpy()\n",
        "\n",
        "\n",
        "# Función para ejecutar la validación\n",
        "def validate_epoch(model, val_dataset, mse_fn):\n",
        "    val_mse_metric = tf.keras.metrics.Mean(name='val_mse')\n",
        "\n",
        "    for val_x, val_y in val_dataset:\n",
        "        # Preprocesar y_batch\n",
        "        principal_components, gaussians = preprocess_ybatch(val_y)\n",
        "\n",
        "        # El modelo debe devolver las dos ramas: lowfreq y highfreq\n",
        "        lowfreq_pred, highfreq_pred = model(val_x, training=False)\n",
        "\n",
        "        # Calcular pérdidas separadas para ambas ramas\n",
        "        mse_loss_low = mse_fn(principal_components, lowfreq_pred)\n",
        "        mse_loss_high = mse_fn(gaussians, highfreq_pred)\n",
        "\n",
        "        # Sumar las pérdidas\n",
        "        total_loss = mse_loss_low + mse_loss_high\n",
        "\n",
        "        # Actualizar la métrica MSE\n",
        "        val_mse_metric(total_loss)\n",
        "\n",
        "    return val_mse_metric.result().numpy()\n",
        "\n",
        "# Función para reiniciar métricas\n",
        "def reset_metrics(metric):\n",
        "    metric.reset_state()\n",
        "\n",
        "\n",
        "# Más sobre metricas\n",
        "import grav_lens.metrics as metrics\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def validate_epoch_with_metrics(model, val_dataset, mse_fn, num_samples_to_take=15):\n",
        "    val_mse_metric = tf.keras.metrics.Mean(name='val_mse')\n",
        "    # Inicializar métricas adicionales\n",
        "    # wmape_metric = tf.keras.metrics.Mean(name='wmape')\n",
        "    # dice_metric = tf.keras.metrics.Mean(name='dice')\n",
        "    # dpeaks_metric = tf.keras.metrics.Mean(name='dpeaks')\n",
        "\n",
        "    val_sample = val_dataset.shuffle(buffer_size=30).take(num_samples_to_take)\n",
        "    print(\"\\tvalidation shuffle ready\")\n",
        "    for val_x, val_y in val_sample:\n",
        "        # Preprocesar y_batch\n",
        "        principal_components, gaussians = preprocess_ybatch(val_y)\n",
        "\n",
        "        # El modelo debe devolver las dos ramas: lowfreq y highfreq\n",
        "        lowfreq_pred, highfreq_pred = model(val_x, training=False)\n",
        "\n",
        "        # Calcular pérdidas separadas para ambas ramas\n",
        "        mse_loss_low = mse_fn(principal_components, lowfreq_pred)\n",
        "        mse_loss_high = mse_fn(gaussians, highfreq_pred)\n",
        "\n",
        "        # Calcular la pérdida de regularización para las desviaciones estándar\n",
        "        std_x_pred = highfreq_pred[..., 2]  # std_x valores predichos\n",
        "        std_y_pred = highfreq_pred[..., 3]  # std_y valores predichos\n",
        "        reg_loss = std_regularization_loss(std_x_pred, std_y_pred)\n",
        "\n",
        "        # Sumar las pérdidas\n",
        "        total_loss = mse_loss_low + mse_loss_high  + reg_loss\n",
        "\n",
        "        # Actualizar la métrica MSE\n",
        "        val_mse_metric(total_loss)\n",
        "    #     # reconstruyo la iamgen\n",
        "    #     model_image_prediction = reconstruct_batch_images(lowfreq_pred.numpy(), highfreq_pred.numpy(), batch_size=64, ipca_low=ipca_low)\n",
        "    #     model_image_prediction = tf.cast(model_image_prediction, dtype=tf.float32)\n",
        "\n",
        "    #     # print(model_image_prediction.shape)\n",
        "    #     # print(val_y.shape)\n",
        "    #     # print(model_image_prediction.dtype)\n",
        "\n",
        "    #     # Calcular métricas adicionales\n",
        "    #     wmape_val = metrics.WMAPE(val_y, model_image_prediction)\n",
        "    #     dice_val = metrics.DICEE(val_y, model_image_prediction)\n",
        "    #     #dpeaks_val = metrics.DPEAKS(val_y.numpy(), model_image_prediction.numpy(), num_peaks=3)\n",
        "\n",
        "    #     # Actualizar las métricas adicionales\n",
        "    #     wmape_metric(wmape_val)\n",
        "    #     dice_metric(dice_val)\n",
        "    #     #dpeaks_metric(dpeaks_val)\n",
        "\n",
        "    return val_mse_metric.result().numpy() #, wmape_metric.result().numpy(), dice_metric.result().numpy() #, dpeaks_metric.result().numpy()\n",
        "\n",
        "def train_model_with_metrics(epochs, model, train_dataset, val_dataset, optimizer, mse_fn, print_steps, callbacks=None, subset_size=100):\n",
        "    # Inicializar las métricas\n",
        "    train_mse_metric = tf.keras.metrics.Mean(name='train_mse')\n",
        "    val_mse_metric = tf.keras.metrics.Mean(name='val_mse')\n",
        "\n",
        "    history = {\n",
        "        'loss': [], 'val_loss': [], 'learning_rate': []\n",
        "    }\n",
        "\n",
        "    # Preparar callbacks\n",
        "    callbacks = prepare_callbacks(callbacks, model)\n",
        "\n",
        "    # Iterar sobre las épocas\n",
        "    for epoch in range(epochs):\n",
        "        print(f'Epoch {epoch + 1}/{epochs}')\n",
        "\n",
        "        # Toma un subset de los datos\n",
        "        subset_dataset = train_dataset.shuffle(buffer_size=300).take(subset_size)\n",
        "\n",
        "        # Entrenar una época\n",
        "\n",
        "        train_loss, train_lr = train_epoch(model, subset_dataset, optimizer, mse_fn, print_steps)\n",
        "        history['loss'].append(train_loss)\n",
        "        history['learning_rate'].append(train_lr)\n",
        "\n",
        "\n",
        "        # Validación con métricas adicionales\n",
        "        val_loss = validate_epoch_with_metrics(model, val_dataset, mse_fn) #, wmape_val, dice_val\n",
        "        history['val_loss'].append(val_loss)\n",
        "\n",
        "        logs = {\n",
        "            'loss': train_loss, 'val_loss': val_loss, 'learning_rate': train_lr\n",
        "        }\n",
        "\n",
        "        run_callbacks_on_epoch_end(callbacks, epoch, logs)\n",
        "\n",
        "        # Resetear métricas\n",
        "\n",
        "        reset_metrics(train_mse_metric)\n",
        "        reset_metrics(val_mse_metric)\n",
        "\n",
        "    # Ejecutar callbacks al final del entrenamiento\n",
        "    print(\"end training callbacks\")\n",
        "    run_callbacks_on_train_end(callbacks)\n",
        "\n",
        "    return history\n",
        "\n",
        "def plot_extended_training_history(history):\n",
        "    \"\"\"\n",
        "    Visualiza la historia de entrenamiento y validación, incluyendo learning rate y métricas adicionales.\n",
        "\n",
        "    Parámetros:\n",
        "        history (dict): Diccionario con 'loss', 'val_loss', 'learning_rate'\n",
        "    \"\"\"\n",
        "    epochs = range(1, len(history['loss']) + 1)\n",
        "\n",
        "    # Crear figura con subplots\n",
        "    fig, ax1 = plt.subplots(figsize=(14, 8))\n",
        "\n",
        "    # Gráfico de las pérdidas\n",
        "    ax1.plot(epochs, history['loss'], 'b-', label='Training Loss')\n",
        "    ax1.plot(epochs, history['val_loss'], 'orange', label='Validation Loss')\n",
        "    ax1.set_xlabel('Epochs')\n",
        "    ax1.set_ylabel('Loss')\n",
        "    ax1.set_title('Training & Validation Loss')\n",
        "    ax1.legend(loc='upper left')\n",
        "\n",
        "    # Crear un segundo eje para el learning rate\n",
        "    ax2 = ax1.twinx()\n",
        "    ax2.plot(epochs, history['learning_rate'], 'gray', label='Learning Rate', linestyle='--')\n",
        "    ax2.set_ylabel('Learning Rate')\n",
        "    ax2.legend(loc='upper right')\n",
        "\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aqui el uso de callbacks"
      ],
      "metadata": {
        "id": "2byvOewO2-HR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Early stopping callback basado en val_mse\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    mode=\"min\",\n",
        "    patience=5,  # Número de épocas sin mejora antes de detener\n",
        "    restore_best_weights=True  # Restaurar los mejores pesos\n",
        ")\n",
        "\n",
        "\n",
        "# Detener después de 1 hora (3600 segundos)\n",
        "# time_stopping = TimeStopping(max_seconds=3600)\n",
        "\n",
        "callbacks = [\n",
        "    early_stopping\n",
        "]\n",
        "\n",
        "# Ejemplo de uso del modelo\n",
        "\n",
        "cnn_model = create_complex_cnn_model()\n",
        "# cnn_model.summary()\n",
        "\n",
        "#from tensorflow.keras.utils import plot_model\n",
        "#plot_model(cnn_model, show_shapes=True, show_layer_names=True)\n"
      ],
      "metadata": {
        "id": "JCIfy0eOvMgy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13af1719-a5aa-4188-a9fc-b6fb377fd86e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/activations/leaky_relu.py:41: UserWarning: Argument `alpha` is deprecated. Use `negative_slope` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# en caso de tener una history previa\n",
        "# prev_history = history"
      ],
      "metadata": {
        "id": "Ek8kKtr8z6s3"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QY5_v08kg17E",
        "outputId": "d52fd6b8-757d-4510-df35-4d91aa6165e3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "Step 0: Total MSE = 700.981, \tLowFreq MSE = 2.104, \tHighFreq MSE = 698.666, \tReg Loss = 0.210, \tLR = 1.000e-03\n",
            "Step 5: Total MSE = 717.969, \tLowFreq MSE = 2.010, \tHighFreq MSE = 731.849, \tReg Loss = 0.527, \tLR = 1.000e-03\n",
            "Step 10: Total MSE = 712.791, \tLowFreq MSE = 1.860, \tHighFreq MSE = 701.175, \tReg Loss = 0.520, \tLR = 1.000e-03\n",
            "Step 15: Total MSE = 703.676, \tLowFreq MSE = 1.973, \tHighFreq MSE = 690.631, \tReg Loss = 0.424, \tLR = 9.999e-04\n",
            "Step 20: Total MSE = 696.691, \tLowFreq MSE = 1.867, \tHighFreq MSE = 654.502, \tReg Loss = 0.531, \tLR = 9.999e-04\n",
            "Step 25: Total MSE = 691.446, \tLowFreq MSE = 1.866, \tHighFreq MSE = 661.395, \tReg Loss = 0.238, \tLR = 9.999e-04\n",
            "Step 30: Total MSE = 686.456, \tLowFreq MSE = 1.782, \tHighFreq MSE = 656.698, \tReg Loss = 0.421, \tLR = 9.998e-04\n",
            "Step 35: Total MSE = 682.492, \tLowFreq MSE = 1.826, \tHighFreq MSE = 639.245, \tReg Loss = 0.239, \tLR = 9.997e-04\n",
            "Step 40: Total MSE = 678.611, \tLowFreq MSE = 1.764, \tHighFreq MSE = 656.778, \tReg Loss = 0.171, \tLR = 9.996e-04\n",
            "Step 45: Total MSE = 674.921, \tLowFreq MSE = 1.645, \tHighFreq MSE = 645.566, \tReg Loss = 0.115, \tLR = 9.996e-04\n",
            "Step 50: Total MSE = 671.390, \tLowFreq MSE = 1.602, \tHighFreq MSE = 625.280, \tReg Loss = 0.509, \tLR = 9.995e-04\n",
            "Step 55: Total MSE = 669.057, \tLowFreq MSE = 1.642, \tHighFreq MSE = 646.602, \tReg Loss = 0.324, \tLR = 9.993e-04\n",
            "\tvalidation shuffle ready\n",
            "Epoch 2/10\n",
            "Step 0: Total MSE = 655.907, \tLowFreq MSE = 1.584, \tHighFreq MSE = 654.212, \tReg Loss = 0.110, \tLR = 9.977e-04\n",
            "Step 5: Total MSE = 642.088, \tLowFreq MSE = 1.640, \tHighFreq MSE = 637.699, \tReg Loss = 0.458, \tLR = 9.975e-04\n",
            "Step 10: Total MSE = 637.100, \tLowFreq MSE = 1.377, \tHighFreq MSE = 635.664, \tReg Loss = 0.257, \tLR = 9.973e-04\n",
            "Step 15: Total MSE = 637.012, \tLowFreq MSE = 1.399, \tHighFreq MSE = 629.937, \tReg Loss = 0.148, \tLR = 9.971e-04\n",
            "Step 20: Total MSE = 634.746, \tLowFreq MSE = 1.516, \tHighFreq MSE = 622.693, \tReg Loss = 0.203, \tLR = 9.969e-04\n",
            "Step 25: Total MSE = 634.259, \tLowFreq MSE = 1.315, \tHighFreq MSE = 631.755, \tReg Loss = 0.093, \tLR = 9.967e-04\n",
            "Step 30: Total MSE = 633.641, \tLowFreq MSE = 1.259, \tHighFreq MSE = 628.019, \tReg Loss = 0.257, \tLR = 9.964e-04\n",
            "Step 35: Total MSE = 633.801, \tLowFreq MSE = 1.293, \tHighFreq MSE = 638.338, \tReg Loss = 0.220, \tLR = 9.962e-04\n",
            "Step 40: Total MSE = 632.499, \tLowFreq MSE = 1.149, \tHighFreq MSE = 613.345, \tReg Loss = 0.386, \tLR = 9.959e-04\n",
            "Step 45: Total MSE = 632.016, \tLowFreq MSE = 1.151, \tHighFreq MSE = 626.431, \tReg Loss = 0.338, \tLR = 9.956e-04\n",
            "Step 50: Total MSE = 630.957, \tLowFreq MSE = 1.138, \tHighFreq MSE = 621.917, \tReg Loss = 0.310, \tLR = 9.954e-04\n",
            "Step 55: Total MSE = 630.378, \tLowFreq MSE = 1.112, \tHighFreq MSE = 627.325, \tReg Loss = 0.326, \tLR = 9.951e-04\n",
            "\tvalidation shuffle ready\n",
            "Epoch 3/10\n",
            "Step 0: Total MSE = 616.799, \tLowFreq MSE = 1.131, \tHighFreq MSE = 615.194, \tReg Loss = 0.475, \tLR = 9.911e-04\n",
            "Step 5: Total MSE = 622.239, \tLowFreq MSE = 0.934, \tHighFreq MSE = 619.183, \tReg Loss = 0.189, \tLR = 9.907e-04\n"
          ]
        }
      ],
      "source": [
        "# Definir el optimizador\n",
        "\n",
        "scheduler = tf.keras.optimizers.schedules.CosineDecay(\n",
        "    initial_learning_rate=1e-3,\n",
        "    decay_steps=2000,  # Número de pasos antes de llegar a lr mínimo\n",
        "    alpha=1e-2  # Valor final del learning rate (porcentaje del learning rate inicial)\n",
        ")\n",
        "\n",
        "optimizer = get_optimizer_with_scheduler('adam', 1e-3, scheduler, momentum=0.1, nesterov=True)\n",
        "\n",
        "# Definir la función de pérdida MSE\n",
        "mse_fn = tf.keras.losses.MeanSquaredError()\n",
        "\n",
        "# Entrenar el modelo\n",
        "# history = train_model(\n",
        "#     epochs=5,                        # Número de épocas de entrenamiento\n",
        "#     model=cnn_model,                  # Modelo CNN que has definido con dos ramas\n",
        "#     train_dataset=train_dataset,      # Dataset de entrenamiento preprocesado\n",
        "#     val_dataset=val_dataset,          # Dataset de validación preprocesado\n",
        "#     optimizer=optimizer,              # Optimizador con el scheduler\n",
        "#     mse_fn=mse_fn,                    # Función de pérdida (MSE)\n",
        "#     print_steps=25,                   # Mostrar progreso cada 10 steps\n",
        "#     callbacks=callbacks               # Callbacks, incluyendo EarlyStopping y TimeStopping\n",
        "# )\n",
        "\n",
        "history = train_model_with_metrics(\n",
        "    epochs=10,\n",
        "    model=cnn_model,\n",
        "    train_dataset=train_dataset, val_dataset=val_dataset,\n",
        "    optimizer=optimizer, mse_fn=mse_fn,\n",
        "    print_steps=5,\n",
        "    callbacks=callbacks,\n",
        "    subset_size=60) # subset es el numero de datos por eqpoca\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-bWRDgG6z33a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# historial"
      ],
      "metadata": {
        "id": "LQkbPcUUtfjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_extended_training_history(history)\n"
      ],
      "metadata": {
        "id": "sYyN-OWwjBtX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWuh2UyPg3iG"
      },
      "source": [
        "### reconstruccion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ENg9cyXpg6id"
      },
      "outputs": [],
      "source": [
        "from grav_lens.preprocess.gmm import reconstruct_image_from_gmm, reconstruct_lowfreq_from_pca, reconstruct_highfreq_from_gmm, reconstruct_batch_images, plot_comparison\n",
        "\n",
        "for X_batch, y_batch in train_dataset.take(1):\n",
        "    y_batch = minmaxscaler.transform(y_batch) # [0, 1] #escalar los datos\n",
        "\n",
        "\n",
        "    low_batch, high_batch = process_batch_filters(y_batch)\n",
        "\n",
        "    predictions = cnn_model.predict(X_batch)\n",
        "    # Obtener el tamaño del batch\n",
        "    batch_size = X_batch.shape[0]\n",
        "    # Reconstruir imágenes de baja y alta frecuencia\n",
        "    reconstructed_images = reconstruct_batch_images(*predictions, batch_size, ipca_low)\n",
        "\n",
        "    # para comparar\n",
        "    real_gaussians = gp.gmm_batch_vectors(high_batch, 40, 0, 1)\n",
        "\n",
        "# graficar\n",
        "for i in range(4):\n",
        "    plot_comparison(y_batch[i,:,:,0],reconstructed_images[i,:,:,0])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Guardar el modelo"
      ],
      "metadata": {
        "id": "XRNTfcIqzUGP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_model.save(\"0930_11pm.keras\")"
      ],
      "metadata": {
        "id": "JeTNqD51zUrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Histograma de Valores"
      ],
      "metadata": {
        "id": "UD4R4m8HQg0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "\n",
        "def gaussians_histogram(predictions_example):\n",
        "    # Suponiendo que `predictions[1]` es la salida de las gaussianas con la forma (64, 40, 5)\n",
        "    # Crear un histograma para cada componente\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20, 4))\n",
        "    component_names = ['mean_x', 'mean_y', 'std_x', 'std_y', 'weight']\n",
        "\n",
        "    for i in range(5):\n",
        "        axes[i].hist(predictions_example[:, :, i].flatten(), bins=50, alpha=0.7, color='blue')\n",
        "        axes[i].set_title(f'Histogram of {component_names[i]}')\n",
        "        axes[i].set_xlabel('Value')\n",
        "        axes[i].set_ylabel('Frequency')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "T4E3lzCpQhj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# datos reales\n",
        "gaussians_histogram(predictions_example = real_gaussians)"
      ],
      "metadata": {
        "id": "L1h55h6aQnYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predichos\n",
        "gaussians_histogram(predictions_example = predictions[1])"
      ],
      "metadata": {
        "id": "NpmrhChGQmE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "I6EXthjzQpoA"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}