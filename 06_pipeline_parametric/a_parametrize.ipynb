{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<grav_lens.models.load_models.CustomMinMaxScaler object at 0x0000026EF3DB18D0>\n",
      "IncrementalPCA(batch_size=64, n_components=64)\n",
      "Using data folder: ..\\data\\1\n",
      "(64, 128, 128, 3)\n",
      "(64, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "from grav_lens.models.load_models import load_minmaxscaler, load_ipca_low\n",
    "# Cargar el MinMaxScaler\n",
    "minmaxscaler = load_minmaxscaler()\n",
    "print(minmaxscaler)\n",
    "# Cargar el modelo IPCA low\n",
    "ipca_low = load_ipca_low()\n",
    "print(ipca_low)\n",
    "\n",
    "\n",
    "from grav_lens import get_datasets\n",
    "max_files = 2000\n",
    "batch_size = 64\n",
    "home_data = os.path.join(\"..\",\"data\")\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = get_datasets(\n",
    "    data_index=1,\n",
    "    max_files=max_files,\n",
    "    home=home_data,\n",
    "    batch_size=batch_size,\n",
    "    val_split=0.2,\n",
    "    test_split=0.1,\n",
    ")\n",
    "\n",
    "for X_batch, y_batch in train_dataset.take(1):\n",
    "    print(X_batch.shape)\n",
    "    print(y_batch.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import grav_lens.preprocess as gp\n",
    "from grav_lens.utils.statistics import get_stats\n",
    "from grav_lens.preprocess.filtering import process_batch_filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini tutorial de las funciones más usadas\n",
    "Algunas funciones de utilidad\n",
    "- gp.apply_threshold\n",
    "- gp.gmm_batch_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se puede separar por encima o abjao de un punto de corte\n",
    "batch_positive, batch_negative =   gp.apply_threshold(y_batch, *get_stats(y_batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separa en baja y alta frecuencia 5.2 s por batch\n",
    "low_batch, high_batch = process_batch_filters(y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Los componentes de baja frecuencia son bien aproximados por componentes principales\n",
    "- Los componentes de alta frecuencia son bien aproximados por peaks gaussianos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31mSignature:\u001b[0m\n",
      "\u001b[0mgp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgmm_batch_vectors\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_gaussians_positive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mn_gaussians_negative\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m    \u001b[0mthreshold\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\n",
      "\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDocstring:\u001b[0m\n",
      "Aplica modelos de mezclas gaussianas (GMM) a un batch de imágenes, generando vectores de medias, desviaciones estándar y pesos para cada imagen.\n",
      "\n",
      "Parámetros:\n",
      "    batch (numpy array): Batch de imágenes a procesar (batch_size, altura, anchura, 1).\n",
      "    n_gaussians_positive (int, opcional): Número de componentes gaussianas positivas (por defecto 30).\n",
      "    n_gaussians_negative (int, opcional): Número de componentes gaussianas negativas (por defecto 10).\n",
      "    threshold (float, opcional): Umbral para separar las frecuencias (por defecto 2).\n",
      "\n",
      "Retorna:\n",
      "    combined_batch (numpy array): Batch de vectores combinados de medias, desviaciones estándar y pesos de cada imagen,\n",
      "                                  con shape (batch_size, n_gaussianas, 5). El vector tiene el formato \n",
      "                                  [mean_x, mean_y, std_x, std_y, weight] para cada gaussiana.\n",
      "\u001b[1;31mFile:\u001b[0m      c:\\users\\fbien.desktop-6fmear7\\desktop\\grav_lensing\\src\\grav_lens\\preprocess\\gmm.py\n",
      "\u001b[1;31mType:\u001b[0m      function"
     ]
    }
   ],
   "source": [
    "gp.gmm_batch_vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(64, 40, 5)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 15 s por batch\n",
    "# (batch, 40, 5) \n",
    "# 40 es el total de gaussianas\n",
    "# 5 el vector correspondiente a cada gaussiana\n",
    "gaussians = gp.gmm_batch_vectors(high_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# El proceso completo\n",
    "De manera que tenemos las dos formas principales de datos\n",
    "- gaussians (batch, 40, 5), con 40 el total de gaussianas, 5 el largo del vector de info: \n",
    "    meanx, meany, stdx, stdy, coef\n",
    "- principal_components (batch, 64), con 64 coeficientes que representan cuanto hay de cada componente\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 128, 128, 3)\n",
      "(64, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in train_dataset.take(1):\n",
    "    y_batch = minmaxscaler.transform(y_batch) # [0, 1] #escalar los datos\n",
    "\n",
    "    print(X_batch.shape)\n",
    "    print(y_batch.shape)\n",
    "    \n",
    "    low_batch, high_batch = process_batch_filters(y_batch)\n",
    "\n",
    "    gaussians = gp.gmm_batch_vectors(high_batch)\n",
    "\n",
    "    low_freq_stack = np.vstack([img.reshape(-1, 128*128) for img in low_batch]) #stack para el pca\n",
    "    principal_components = ipca_low.transform(low_freq_stack)\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
