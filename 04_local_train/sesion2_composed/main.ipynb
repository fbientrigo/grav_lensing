{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 02_main.ipynb\n",
    "from grav_lens import get_datasets\n",
    "\n",
    "from utils.model import create_model\n",
    "from utils.loadsave import load_model_with_hyperparameters, load_hyperparameters, save_hyperparameters\n",
    "from utils.optimize import dimensions, default_parameters\n",
    "\n",
    "\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "from skopt.utils import use_named_args\n",
    "from skopt.callbacks import CheckpointSaver\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using data folder: ..\\..\\data\\1\n",
      "Train X: (32, 128, 128, 3)\n",
      "Train Y: (32, 128, 128, 1)\n",
      "Val X: (12, 128, 128, 3)\n",
      "Val Y: (12, 128, 128, 1)\n",
      "Test X: (6, 128, 128, 3)\n",
      "Test Y: (6, 128, 128, 1)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "# mi carpeta data se encuentra en el root\n",
    "home_data = os.path.join(\"..\", \"..\")\n",
    "# Ejemplo de uso\n",
    "# INDEX 0 es una carpeta de datasets mucho mas chica\n",
    "train_dataset, val_dataset, test_dataset = get_datasets(data_index='1', max_files=60, home=home_data)\n",
    "\n",
    "for X, Y in train_dataset.take(1):  # Mostrar un batch de entrenamiento\n",
    "    print(\"Train X:\", X.shape)\n",
    "    print(\"Train Y:\", Y.shape)\n",
    "\n",
    "for X, Y in val_dataset.take(1):  # Mostrar un batch de validación\n",
    "    print(\"Val X:\", X.shape)\n",
    "    print(\"Val Y:\", Y.shape)\n",
    "\n",
    "for X, Y in test_dataset.take(1):  # Mostrar un batch de prueba\n",
    "    print(\"Test X:\", X.shape)\n",
    "    print(\"Test Y:\", Y.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definiendo Funcion Optimizacion\n",
    "Se ha intentado modularizar pero resulta en problems de pickling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_loss = float('inf')\n",
    "counter = 1\n",
    "@use_named_args(dimensions=dimensions)\n",
    "def F_objective(learning_rate, \n",
    "     in_activation, h_activation, out_activation, \n",
    "     h_kernel_size, hidden_filters, \n",
    "     out_kernel_size, weight_kl, \n",
    "     beta_1, beta_2, epsilon, amsgrad, \n",
    "     decay_steps, decay_rate, epochs):\n",
    "    \"\"\"\n",
    "    Función objetivo para la optimización de hiperparámetros.\n",
    "    \"\"\"\n",
    "\n",
    "    model = create_model(learning_rate, \n",
    "                        in_activation, h_activation, \n",
    "                        out_activation, h_kernel_size, \n",
    "                        hidden_filters, out_kernel_size, \n",
    "                        weight_kl, beta_1, beta_2, \n",
    "                        epsilon, amsgrad, \n",
    "                        decay_steps, decay_rate)\n",
    "\n",
    "    model.fit(train_dataset, epochs=epochs, verbose=True)\n",
    "\n",
    "    loss = model.evaluate(val_dataset, verbose=False)\n",
    "\n",
    "    print(f\"\\nLoss: {loss:.2%}\\n\")\n",
    "\n",
    "    if loss < best_loss:\n",
    "        model.save_weights(f'best_model_{counter}.weights.h5')\n",
    "        save_hyperparameters(\n",
    "            learning_rate, in_activation, \n",
    "            h_activation, out_activation, \n",
    "            h_kernel_size, hidden_filters, \n",
    "            out_kernel_size, weight_kl, \n",
    "            beta_1, beta_2, epsilon, \n",
    "            amsgrad, decay_steps, \n",
    "            decay_rate, optimizer.counter\n",
    "        )\n",
    "        print(f\"Model weights and hyperparameters saved with ID: {optimizer.counter}\")\n",
    "        optimizer.counter += 1\n",
    "        optimizer.best_loss = loss\n",
    "\n",
    "    K.clear_session()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyp_optimize(dimensions, default_parameters, n_calls=17):\n",
    "    checkpoint_saver = CheckpointSaver(\"checkpoint.pkl\", compress=9)\n",
    "\n",
    "    start_time = time.time()\n",
    "    res = gp_minimize(\n",
    "        func=F_objective,  # Pasar la función parcial\n",
    "        dimensions=dimensions,\n",
    "        acq_func='EI', \n",
    "        n_calls=n_calls,\n",
    "        x0=default_parameters,\n",
    "        callback=[checkpoint_saver]\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    execution_time_minutes = (end_time - start_time) / 60\n",
    "    print(f\"Execution time: {execution_time_minutes:.2f} minutes\")\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test optimize, corre una unica vez la funcion F a optimizar\n",
    "# hyp_optimizer.run_test_optimize(train_dataset, val_dataset, verbose_train=True, verbose_val=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# La Gran Minimizacion\n",
    "La siguiente funcion correra aproximadamente `n_calls` * el tiempo que corrio el paso anterior, debes tener en cuenta si se entrenan epocas y otras que aumenten el tiempo de manera no lineal dependiendo de la accion del hiperparametro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = run_hyp_optimize(dimensions, default_parameters, n_calls=17)\n",
    "\n",
    "# Guardar el objeto res, para hacer estadistica despues\n",
    "with open('optimization_results.pkl', 'wb') as f:\n",
    "    pickle.dump(res, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "from skopt.plots import plot_objective, plot_evaluations\n",
    "from skopt.plots import plot_histogram, plot_objective_2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_convergence(res)\n",
    "print(res.space)\n",
    "print(res.x)\n",
    "print(\"Accuracy: \", res.fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_histogram, plot_objective_2D\n",
    "\n",
    "plot_histogram(result=res,  dimension_identifier='activation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_objective(result=res, dimensions=dimension_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_evaluations(result=res, dimensions=dimension_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
